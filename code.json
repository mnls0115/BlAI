{
    "model.py" : "# Copyright (c) Meta Platforms, Inc. and affiliates.\n# This software may be used and distributed in accordance with the terms of the Llama 3 Community License Agreement.\n\nimport math\nfrom dataclasses import dataclass\nfrom typing import Optional, Tuple\n\nimport fairscale.nn.model_parallel.initialize as fs_init\nimport torch\nimport torch.nn.functional as F\nfrom fairscale.nn.model_parallel.layers import (\nColumnParallelLinear,\nRowParallelLinear,\nVocabParallelEmbedding,\n)\nfrom torch import nn\n\n\n@dataclass\nclass ModelArgs:\ndim: int = 4096\nn_layers: int = 32\nn_heads: int = 32\nn_kv_heads: Optional[int] = None\nvocab_size: int = -1\nmultiple_of: int = 256  # make SwiGLU hidden layer size multiple of large power of 2\nffn_dim_multiplier: Optional[float] = None\nnorm_eps: float = 1e-5\nrope_theta: float = 500000\n\nmax_batch_size: int = 32\nmax_seq_len: int = 2048\n\n\nclass RMSNorm(torch.nn.Module):\ndef __init__(self, dim: int, eps: float = 1e-6):\nsuper().__init__()\nself.eps = eps\nself.weight = nn.Parameter(torch.ones(dim))\n\ndef _norm(self, x):\nreturn x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n\ndef forward(self, x):\noutput = self._norm(x.float()).type_as(x)\nreturn output * self.weight\n\n\ndef precompute_freqs_cis(dim: int, end: int, theta: float = 10000.0):\nfreqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\nt = torch.arange(end, device=freqs.device, dtype=torch.float32)\nfreqs = torch.outer(t, freqs)\nfreqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64\nreturn freqs_cis\n\n\ndef reshape_for_broadcast(freqs_cis: torch.Tensor, x: torch.Tensor):\nndim = x.ndim\nassert 0 <= 1 < ndim\nassert freqs_cis.shape == (x.shape[1], x.shape[-1])\nshape = [d if i == 1 or i == ndim - 1 else 1 for i, d in enumerate(x.shape)]\nreturn freqs_cis.view(*shape)\n\n\ndef apply_rotary_emb(\nxq: torch.Tensor,\nxk: torch.Tensor,\nfreqs_cis: torch.Tensor,\n) -> Tuple[torch.Tensor, torch.Tensor]:\nxq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))\nxk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2))\nfreqs_cis = reshape_for_broadcast(freqs_cis, xq_)\nxq_out = torch.view_as_real(xq_ * freqs_cis).flatten(3)\nxk_out = torch.view_as_real(xk_ * freqs_cis).flatten(3)\nreturn xq_out.type_as(xq), xk_out.type_as(xk)\n\n\ndef repeat_kv(x: torch.Tensor, n_rep: int) -> torch.Tensor:\n\"\"\"torch.repeat_interleave(x, dim=2, repeats=n_rep)\"\"\"\nbs, slen, n_kv_heads, head_dim = x.shape\nif n_rep == 1:\nreturn x\nreturn (\nx[:, :, :, None, :]\n.expand(bs, slen, n_kv_heads, n_rep, head_dim)\n.reshape(bs, slen, n_kv_heads * n_rep, head_dim)\n)\n\n\nclass Attention(nn.Module):\ndef __init__(self, args: ModelArgs):\nsuper().__init__()\nself.n_kv_heads = args.n_heads if args.n_kv_heads is None else args.n_kv_heads\nmodel_parallel_size = fs_init.get_model_parallel_world_size()\nself.n_local_heads = args.n_heads // model_parallel_size\nself.n_local_kv_heads = self.n_kv_heads // model_parallel_size\nself.n_rep = self.n_local_heads // self.n_local_kv_heads\nself.head_dim = args.dim // args.n_heads\n\nself.wq = ColumnParallelLinear(\nargs.dim,\nargs.n_heads * self.head_dim,\nbias=False,\ngather_output=False,\ninit_method=lambda x: x,\n)\nself.wk = ColumnParallelLinear(\nargs.dim,\nself.n_kv_heads * self.head_dim,\nbias=False,\ngather_output=False,\ninit_method=lambda x: x,\n)\nself.wv = ColumnParallelLinear(\nargs.dim,\nself.n_kv_heads * self.head_dim,\nbias=False,\ngather_output=False,\ninit_method=lambda x: x,\n)\nself.wo = RowParallelLinear(\nargs.n_heads * self.head_dim,\nargs.dim,\nbias=False,\ninput_is_parallel=True,\ninit_method=lambda x: x,\n)\n\nself.cache_k = torch.zeros(\n(\nargs.max_batch_size,\nargs.max_seq_len,\nself.n_local_kv_heads,\nself.head_dim,\n)\n).cuda()\nself.cache_v = torch.zeros(\n(\nargs.max_batch_size,\nargs.max_seq_len,\nself.n_local_kv_heads,\nself.head_dim,\n)\n).cuda()\n\ndef forward(\nself,\nx: torch.Tensor,\nstart_pos: int,\nfreqs_cis: torch.Tensor,\nmask: Optional[torch.Tensor],\n):\nbsz, seqlen, _ = x.shape\nxq, xk, xv = self.wq(x), self.wk(x), self.wv(x)\n\nxq = xq.view(bsz, seqlen, self.n_local_heads, self.head_dim)\nxk = xk.view(bsz, seqlen, self.n_local_kv_heads, self.head_dim)\nxv = xv.view(bsz, seqlen, self.n_local_kv_heads, self.head_dim)\n\nxq, xk = apply_rotary_emb(xq, xk, freqs_cis=freqs_cis)\n\nself.cache_k = self.cache_k.to(xq)\nself.cache_v = self.cache_v.to(xq)\n\nself.cache_k[:bsz, start_pos : start_pos + seqlen] = xk\nself.cache_v[:bsz, start_pos : start_pos + seqlen] = xv\n\nkeys = self.cache_k[:bsz, : start_pos + seqlen]\nvalues = self.cache_v[:bsz, : start_pos + seqlen]\n\n# repeat k/v heads if n_kv_heads < n_heads\nkeys = repeat_kv(\nkeys, self.n_rep\n)  # (bs, cache_len + seqlen, n_local_heads, head_dim)\nvalues = repeat_kv(\nvalues, self.n_rep\n)  # (bs, cache_len + seqlen, n_local_heads, head_dim)\n\nxq = xq.transpose(1, 2)  # (bs, n_local_heads, seqlen, head_dim)\nkeys = keys.transpose(1, 2)  # (bs, n_local_heads, cache_len + seqlen, head_dim)\nvalues = values.transpose(\n1, 2\n)  # (bs, n_local_heads, cache_len + seqlen, head_dim)\nscores = torch.matmul(xq, keys.transpose(2, 3)) / math.sqrt(self.head_dim)\nif mask is not None:\nscores = scores + mask  # (bs, n_local_heads, seqlen, cache_len + seqlen)\nscores = F.softmax(scores.float(), dim=-1).type_as(xq)\noutput = torch.matmul(scores, values)  # (bs, n_local_heads, seqlen, head_dim)\noutput = output.transpose(1, 2).contiguous().view(bsz, seqlen, -1)\nreturn self.wo(output)\n\n\nclass FeedForward(nn.Module):\ndef __init__(\nself,\ndim: int,\nhidden_dim: int,\nmultiple_of: int,\nffn_dim_multiplier: Optional[float],\n):\nsuper().__init__()\nhidden_dim = int(2 * hidden_dim / 3)\n# custom dim factor multiplier\nif ffn_dim_multiplier is not None:\nhidden_dim = int(ffn_dim_multiplier * hidden_dim)\nhidden_dim = multiple_of * ((hidden_dim + multiple_of - 1) // multiple_of)\n\nself.w1 = ColumnParallelLinear(\ndim, hidden_dim, bias=False, gather_output=False, init_method=lambda x: x\n)\nself.w2 = RowParallelLinear(\nhidden_dim, dim, bias=False, input_is_parallel=True, init_method=lambda x: x\n)\nself.w3 = ColumnParallelLinear(\ndim, hidden_dim, bias=False, gather_output=False, init_method=lambda x: x\n)\n\ndef forward(self, x):\nreturn self.w2(F.silu(self.w1(x)) * self.w3(x))\n\n\nclass TransformerBlock(nn.Module):\ndef __init__(self, layer_id: int, args: ModelArgs):\nsuper().__init__()\nself.n_heads = args.n_heads\nself.dim = args.dim\nself.head_dim = args.dim // args.n_heads\nself.attention = Attention(args)\nself.feed_forward = FeedForward(\ndim=args.dim,\nhidden_dim=4 * args.dim,\nmultiple_of=args.multiple_of,\nffn_dim_multiplier=args.ffn_dim_multiplier,\n)\nself.layer_id = layer_id\nself.attention_norm = RMSNorm(args.dim, eps=args.norm_eps)\nself.ffn_norm = RMSNorm(args.dim, eps=args.norm_eps)\n\ndef forward(\nself,\nx: torch.Tensor,\nstart_pos: int,\nfreqs_cis: torch.Tensor,\nmask: Optional[torch.Tensor],\n):\nh = x + self.attention(self.attention_norm(x), start_pos, freqs_cis, mask)\nout = h + self.feed_forward(self.ffn_norm(h))\nreturn out\n\n\nclass Transformer(nn.Module):\ndef __init__(self, params: ModelArgs):\nsuper().__init__()\nself.params = params\nself.vocab_size = params.vocab_size\nself.n_layers = params.n_layers\n\nself.tok_embeddings = VocabParallelEmbedding(\nparams.vocab_size, params.dim, init_method=lambda x: x\n)\n\nself.layers = torch.nn.ModuleList()\nfor layer_id in range(params.n_layers):\nself.layers.append(TransformerBlock(layer_id, params))\n\nself.norm = RMSNorm(params.dim, eps=params.norm_eps)\nself.output = ColumnParallelLinear(\nparams.dim, params.vocab_size, bias=False, init_method=lambda x: x\n)\n\nself.freqs_cis = precompute_freqs_cis(\nparams.dim // params.n_heads,\nparams.max_seq_len * 2,\nparams.rope_theta,\n)\n\n@torch.inference_mode()\ndef forward(self, tokens: torch.Tensor, start_pos: int):\n_bsz, seqlen = tokens.shape\nh = self.tok_embeddings(tokens)\nself.freqs_cis = self.freqs_cis.to(h.device)\nfreqs_cis = self.freqs_cis[start_pos : start_pos + seqlen]\n\nmask = None\nif seqlen > 1:\nmask = torch.full((seqlen, seqlen), float(\"-inf\"), device=tokens.device)\n\nmask = torch.triu(mask, diagonal=1)\n\n# When performing key-value caching, we compute the attention scores\n# only for the new sequence. Thus, the matrix of scores is of size\n# (seqlen, cache_len + seqlen), and the only masked entries are (i, j) for\n# j > cache_len + i, since row i corresponds to token cache_len + i.\nmask = torch.hstack(\n[torch.zeros((seqlen, start_pos), device=tokens.device), mask]\n).type_as(h)\n\nfor layer in self.layers:\nh = layer(h, start_pos, freqs_cis, mask)\nh = self.norm(h)\noutput = self.output(h).float()\nreturn output",
    "generation.py": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n# This software may be used and distributed in accordance with the terms of the Llama 3 Community License Agreement.\n\nimport json\nimport os\nimport sys\nimport time\nfrom pathlib import Path\nfrom typing import List, Optional, Tuple, TypedDict\n\nimport torch\nimport torch.nn.functional as F\nfrom fairscale.nn.model_parallel.initialize import (\nget_model_parallel_rank,\ninitialize_model_parallel,\nmodel_parallel_is_initialized,\n)\n\nfrom llama.model import ModelArgs, Transformer\nfrom llama.tokenizer import ChatFormat, Dialog, Message, Tokenizer\n\n\nclass CompletionPrediction(TypedDict, total=False):\ngeneration: str\ntokens: List[str]  # not required\nlogprobs: List[float]  # not required\n\n\nclass ChatPrediction(TypedDict, total=False):\ngeneration: Message\ntokens: List[str]  # not required\nlogprobs: List[float]  # not required\n\n\nclass Llama:\n@staticmethod\ndef build(\nckpt_dir: str,\ntokenizer_path: str,\nmax_seq_len: int,\nmax_batch_size: int,\nmodel_parallel_size: Optional[int] = None,\nseed: int = 1,\n) -> \"Llama\":\n\"\"\"\nBuild a Llama instance by initializing and loading a model checkpoint.\n\nArgs:\nckpt_dir (str): Path to the directory containing checkpoint files.\ntokenizer_path (str): Path to the tokenizer file.\nmax_seq_len (int): Maximum sequence length for input text.\nmax_batch_size (int): Maximum batch size for inference.\nmodel_parallel_size (Optional[int], optional): Number of model parallel processes.\nIf not provided, it's determined from the environment. Defaults to None.\n\nReturns:\nLlama: An instance of the Llama class with the loaded model and tokenizer.\n\nRaises:\nAssertionError: If there are no checkpoint files in the specified directory,\nor if the model parallel size does not match the number of checkpoint files.\n\nNote:\nThis method initializes the distributed process group, sets the device to CUDA,\nand loads the pre-trained model and tokenizer.\n\"\"\"\nif not torch.distributed.is_initialized():\ntorch.distributed.init_process_group(\"nccl\")\nif not model_parallel_is_initialized():\nif model_parallel_size is None:\nmodel_parallel_size = int(os.environ.get(\"WORLD_SIZE\", 1))\ninitialize_model_parallel(model_parallel_size)\n\nlocal_rank = int(os.environ.get(\"LOCAL_RANK\", 0))\ntorch.cuda.set_device(local_rank)\n\n# seed must be the same in all processes\ntorch.manual_seed(seed)\n\nif local_rank > 0:\nsys.stdout = open(os.devnull, \"w\")\n\nstart_time = time.time()\ncheckpoints = sorted(Path(ckpt_dir).glob(\"*.pth\"))\nassert len(checkpoints) > 0, f\"no checkpoint files found in {ckpt_dir}\"\nassert model_parallel_size == len(\ncheckpoints\n), f\"Loading a checkpoint for MP={len(checkpoints)} but world size is {model_parallel_size}\"\nckpt_path = checkpoints[get_model_parallel_rank()]\ncheckpoint = torch.load(ckpt_path, map_location=\"cpu\")\nwith open(Path(ckpt_dir) / \"params.json\", \"r\") as f:\nparams = json.loads(f.read())\n\nmodel_args: ModelArgs = ModelArgs(\nmax_seq_len=max_seq_len,\nmax_batch_size=max_batch_size,\n**params,\n)\ntokenizer = Tokenizer(model_path=tokenizer_path)\nassert model_args.vocab_size == tokenizer.n_words\nif torch.cuda.is_bf16_supported():\ntorch.set_default_tensor_type(torch.cuda.BFloat16Tensor)\nelse:\ntorch.set_default_tensor_type(torch.cuda.HalfTensor)\nmodel = Transformer(model_args)\nmodel.load_state_dict(checkpoint, strict=False)\nprint(f\"Loaded in {time.time() - start_time:.2f} seconds\")\n\nreturn Llama(model, tokenizer)\n\ndef __init__(self, model: Transformer, tokenizer: Tokenizer):\nself.model = model\nself.tokenizer = tokenizer\nself.formatter = ChatFormat(tokenizer)\n\n@torch.inference_mode()\ndef generate(\nself,\nprompt_tokens: List[List[int]],\nmax_gen_len: int,\ntemperature: float = 0.6,\ntop_p: float = 0.9,\nlogprobs: bool = False,\necho: bool = False,\n) -> Tuple[List[List[int]], Optional[List[List[float]]]]:\n\"\"\"\nGenerate text sequences based on provided prompts using the language generation model.\n\nArgs:\nprompt_tokens (List[List[int]]): List of tokenized prompts, where each prompt is represented as a list of integers.\nmax_gen_len (int): Maximum length of the generated text sequence.\ntemperature (float, optional): Temperature value for controlling randomness in sampling. Defaults to 0.6.\ntop_p (float, optional): Top-p probability threshold for nucleus sampling. Defaults to 0.9.\nlogprobs (bool, optional): Flag indicating whether to compute token log probabilities. Defaults to False.\necho (bool, optional): Flag indicating whether to include prompt tokens in the generated output. Defaults to False.\n\nReturns:\nTuple[List[List[int]], Optional[List[List[float]]]]: A tuple containing generated token sequences and, if logprobs is True, corresponding token log probabilities.\n\nNote:\nThis method uses the provided prompts as a basis for generating text. It employs nucleus sampling to produce text with controlled randomness.\nIf logprobs is True, token log probabilities are computed for each generated token.\n\n\"\"\"\nparams = self.model.params\nbsz = len(prompt_tokens)\nassert bsz <= params.max_batch_size, (bsz, params.max_batch_size)\n\nmin_prompt_len = min(len(t) for t in prompt_tokens)\nmax_prompt_len = max(len(t) for t in prompt_tokens)\nassert max_prompt_len <= params.max_seq_len\ntotal_len = min(params.max_seq_len, max_gen_len + max_prompt_len)\n\npad_id = self.tokenizer.pad_id\ntokens = torch.full((bsz, total_len), pad_id, dtype=torch.long, device=\"cuda\")\nfor k, t in enumerate(prompt_tokens):\ntokens[k, : len(t)] = torch.tensor(t, dtype=torch.long, device=\"cuda\")\nif logprobs:\ntoken_logprobs = torch.zeros_like(tokens, dtype=torch.float)\n\nprev_pos = 0\neos_reached = torch.tensor([False] * bsz, device=\"cuda\")\ninput_text_mask = tokens != pad_id\nif min_prompt_len == total_len:\nlogits = self.model.forward(tokens, prev_pos)\ntoken_logprobs = -F.cross_entropy(\ninput=logits.transpose(1, 2),\ntarget=tokens,\nreduction=\"none\",\nignore_index=pad_id,\n)\n\nstop_tokens = torch.tensor(list(self.tokenizer.stop_tokens))\n\nfor cur_pos in range(min_prompt_len, total_len):\nlogits = self.model.forward(tokens[:, prev_pos:cur_pos], prev_pos)\nif temperature > 0:\nprobs = torch.softmax(logits[:, -1] / temperature, dim=-1)\nnext_token = sample_top_p(probs, top_p)\nelse:\nnext_token = torch.argmax(logits[:, -1], dim=-1)\n\nnext_token = next_token.reshape(-1)\n# only replace token if prompt has already been generated\nnext_token = torch.where(\ninput_text_mask[:, cur_pos], tokens[:, cur_pos], next_token\n)\ntokens[:, cur_pos] = next_token\nif logprobs:\ntoken_logprobs[:, prev_pos + 1 : cur_pos + 1] = -F.cross_entropy(\ninput=logits.transpose(1, 2),\ntarget=tokens[:, prev_pos + 1 : cur_pos + 1],\nreduction=\"none\",\nignore_index=pad_id,\n)\neos_reached |= (~input_text_mask[:, cur_pos]) & (\ntorch.isin(next_token, stop_tokens)\n)\nprev_pos = cur_pos\nif all(eos_reached):\nbreak\n\nif logprobs:\ntoken_logprobs = token_logprobs.tolist()\nout_tokens, out_logprobs = [], []\nfor i, toks in enumerate(tokens.tolist()):\n# cut to max gen len\nstart = 0 if echo else len(prompt_tokens[i])\ntoks = toks[start : len(prompt_tokens[i]) + max_gen_len]\nprobs = None\nif logprobs:\nprobs = token_logprobs[i][start : len(prompt_tokens[i]) + max_gen_len]\n# cut to after eos tok if any\nfor stop_token in self.tokenizer.stop_tokens:\ntry:\neos_idx = toks.index(stop_token)\ntoks = toks[:eos_idx]\nprobs = probs[:eos_idx] if logprobs else None\nexcept ValueError:\npass\nout_tokens.append(toks)\nout_logprobs.append(probs)\nreturn (out_tokens, out_logprobs if logprobs else None)\n\ndef text_completion(\nself,\nprompts: List[str],\ntemperature: float = 0.6,\ntop_p: float = 0.9,\nmax_gen_len: Optional[int] = None,\nlogprobs: bool = False,\necho: bool = False,\n) -> List[CompletionPrediction]:\n\"\"\"\nPerform text completion for a list of prompts using the language generation model.\n\nArgs:\nprompts (List[str]): List of text prompts for completion.\ntemperature (float, optional): Temperature value for controlling randomness in sampling. Defaults to 0.6.\ntop_p (float, optional): Top-p probability threshold for nucleus sampling. Defaults to 0.9.\nmax_gen_len (Optional[int], optional): Maximum length of the generated completion sequence.\nIf not provided, it's set to the model's maximum sequence length minus 1.\nlogprobs (bool, optional): Flag indicating whether to compute token log probabilities. Defaults to False.\necho (bool, optional): Flag indicating whether to include prompt tokens in the generated output. Defaults to False.\n\nReturns:\nList[CompletionPrediction]: List of completion predictions, each containing the generated text completion.\n\nNote:\nThis method generates text completions for the provided prompts, employing nucleus sampling to introduce controlled randomness.\nIf logprobs is True, token log probabilities are computed for each generated token.\n\n\"\"\"\nif max_gen_len is None:\nmax_gen_len = self.model.params.max_seq_len - 1\nprompt_tokens = [self.tokenizer.encode(x, bos=True, eos=False) for x in prompts]\ngeneration_tokens, generation_logprobs = self.generate(\nprompt_tokens=prompt_tokens,\nmax_gen_len=max_gen_len,\ntemperature=temperature,\ntop_p=top_p,\nlogprobs=logprobs,\necho=echo,\n)\nif logprobs:\nreturn [\n{\n\"generation\": self.tokenizer.decode(t),\n\"tokens\": [self.tokenizer.decode([x]) for x in t],\n\"logprobs\": logprobs_i,\n}\nfor t, logprobs_i in zip(generation_tokens, generation_logprobs)\n]\nreturn [{\"generation\": self.tokenizer.decode(t)} for t in generation_tokens]\n\ndef chat_completion(\nself,\ndialogs: List[Dialog],\ntemperature: float = 0.6,\ntop_p: float = 0.9,\nmax_gen_len: Optional[int] = None,\nlogprobs: bool = False,\n) -> List[ChatPrediction]:\n\"\"\"\nGenerate assistant responses for a list of conversational dialogs using the language generation model.\n\nArgs:\ndialogs (List[Dialog]): List of conversational dialogs, where each dialog is a list of messages.\ntemperature (float, optional): Temperature value for controlling randomness in sampling. Defaults to 0.6.\ntop_p (float, optional): Top-p probability threshold for nucleus sampling. Defaults to 0.9.\nmax_gen_len (Optional[int], optional): Maximum length of the generated response sequence.\nIf not provided, it's set to the model's maximum sequence length minus 1.\nlogprobs (bool, optional): Flag indicating whether to compute token log probabilities. Defaults to False.\n\nReturns:\nList[ChatPrediction]: List of chat predictions, each containing the assistant's generated response.\n\nNote:\nThis method generates assistant responses for the provided conversational dialogs.\nIt employs nucleus sampling to introduce controlled randomness in text generation.\nIf logprobs is True, token log probabilities are computed for each generated token.\n\"\"\"\nif max_gen_len is None:\nmax_gen_len = self.model.params.max_seq_len - 1\n\nprompt_tokens = [\nself.formatter.encode_dialog_prompt(dialog) for dialog in dialogs\n]\ngeneration_tokens, generation_logprobs = self.generate(\nprompt_tokens=prompt_tokens,\nmax_gen_len=max_gen_len,\ntemperature=temperature,\ntop_p=top_p,\nlogprobs=logprobs,\n)\nif logprobs:\nreturn [\n{\n\"generation\": {\n\"role\": \"assistant\",\n\"content\": self.tokenizer.decode(t),\n},\n\"tokens\": [self.tokenizer.decode([x]) for x in t],\n\"logprobs\": logprobs_i,\n}\nfor t, logprobs_i in zip(generation_tokens, generation_logprobs)\n]\nreturn [\n{\n\"generation\": {\n\"role\": \"assistant\",\n\"content\": self.tokenizer.decode(t),\n},\n}\nfor t in generation_tokens\n]\n\n\ndef sample_top_p(probs, p):\n\"\"\"\nPerform top-p (nucleus) sampling on a probability distribution.\n\nArgs:\nprobs (torch.Tensor): Probability distribution tensor.\np (float): Probability threshold for top-p sampling.\n\nReturns:\ntorch.Tensor: Sampled token indices.\n\nNote:\nTop-p sampling selects the smallest set of tokens whose cumulative probability mass\nexceeds the threshold p. The distribution is renormalized based on the selected tokens.\n\"\"\"\nprobs_sort, probs_idx = torch.sort(probs, dim=-1, descending=True)\nprobs_sum = torch.cumsum(probs_sort, dim=-1)\nmask = probs_sum - probs_sort > p\nprobs_sort[mask] = 0.0\nprobs_sort.div_(probs_sort.sum(dim=-1, keepdim=True))\nnext_token = torch.multinomial(probs_sort, num_samples=1)\nnext_token = torch.gather(probs_idx, -1, next_token)\nreturn next_token",
    "requirements.txt": "torch\nfairscale\nfire\ntiktoken==0.4.0\nblobfile"
}